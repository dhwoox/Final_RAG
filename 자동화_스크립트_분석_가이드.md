# 자동화 스크립트 분석 가이드

## 📌 목적
TC 기반 자동화 스크립트를 정확하게 분석하고 평가하기 위한 체크리스트

---

## 🎯 분석 전 필수 확인 사항

### 1. 평가 차이 발생 원인 이해

#### 문제 상황
- **1차 분석 (단독)**: 77.6점 / "양호한 수준"
- **2차 분석 (비교)**: 36점 / "낮음"

#### 원인 분석

##### A. 비교 대상의 존재 여부
| 분석 방식 | 평가 기준 | 결과 |
|----------|----------|------|
| **1차 (절대 평가)** | TC 문서만 참조 | 구조 중심 평가 |
| **2차 (상대 평가)** | 검증된 코드와 비교 | 실행 가능성 중심 평가 |

**교훈:** 같은 TC를 다룬 다른 코드가 있는지 먼저 확인 필요

##### B. 실행 가능성 검증 수준
```python
# 1차 분석: 코드 존재만 확인
def test_something(self):
    self.doSomething()  # ✓ 코드 있음 → "구현됨"

# 2차 분석: API 존재 여부까지 확인
def test_something(self):
    self.doSomething()  # ✗ 메서드 없음 → "동작 불가"
```

**교훈:** 기반 클래스의 실제 메서드 목록 확인 필요

##### C. 참조 코드 발견
- 1차: test/temp/ 만 알고 있음
- 2차: test/ 에 검증된 코드 발견
- **결과:** temp는 실험 코드, test/가 실제 사용 코드

**교훈:** 전체 프로젝트 구조 파악 필요

---

## 📋 분석 전 체크리스트

### ✅ 필수 정보 (Priority 1)

#### 1. 프로젝트 구조 파악
```bash
# 확인 명령어
tree test/ -L 2
ls -la test/

# 목적
- 중복 폴더 존재 여부 (test/, test/temp/, test/backup/)
- 각 폴더의 역할 (production, experimental, archive)
- 우선순위 결정 (어느 코드를 기준으로 삼을 것인가)
```

**체크 항목:**
- [ ] test/ 폴더 구조 확인
- [ ] test/temp/ 폴더 목적 확인
- [ ] 어느 폴더가 실제 사용 코드인지 확인
- [ ] README 또는 문서에서 폴더 설명 찾기

---

#### 2. 기반 클래스 및 API 확인
```bash
# 확인 명령어
grep -n "def " test/testCOMMONR.py | head -30
grep -n "def " test/util.py | head -30

# 목적
- 사용 가능한 메서드 목록
- 상속받은 기능 파악
- API 호출 방식 이해
```

**체크 항목:**
- [ ] testCOMMONR (또는 TestCOMMONR) 클래스 정의 읽기
- [ ] 주요 메서드 목록 확인:
  - [ ] enrollUsers()
  - [ ] getUsers()
  - [ ] addBlacklist()
  - [ ] EventMonitor 사용법
- [ ] util 모듈 함수 목록:
  - [ ] randomAlphanumericUserID()
  - [ ] generateRandomName()
  - [ ] generateRandomPIN()

---

#### 3. TC 문서 원본 확인
```bash
# 파일 위치
TC_COMMONR/COMMONR-XX.txt

# 목적
- 정확한 요구사항 파악
- 각 Step별 세부 사항
- 예상 결과 (Expected Result)
```

**체크 항목:**
- [ ] TC 파일 존재 확인
- [ ] 총 Step 수 파악
- [ ] 각 Step의 세부 절차 이해
- [ ] 예상 결과 명확히 파악

---

#### 4. 기존 구현 코드 존재 여부
```bash
# 확인 명령어
find . -name "*COMMONR_XX*" -type f
git log --oneline --all -- test/*COMMONR_XX*

# 목적
- 중복 작업 방지
- 비교 기준 코드 확인
- 버전 이력 파악
```

**체크 항목:**
- [ ] 같은 TC 번호의 다른 파일 검색
- [ ] 각 파일의 작성 날짜 확인
- [ ] 각 파일의 코드 라인 수 비교
- [ ] Git 커밋 이력 확인 (있다면)

---

### 🔸 권장 정보 (Priority 2)

#### 5. 코딩 가이드 확인
```bash
# 확인 파일
CLAUDE.md
README.md
CONTRIBUTING.md

# 목적
- 코드 스타일 규칙
- 금지 사항 (setUp/tearDown 재정의 등)
- 필수 포함 사항
```

**체크 항목:**
- [ ] CLAUDE.md 읽기
- [ ] 자동화 스크립트 작성 규칙 확인
- [ ] 금지된 코드 패턴 파악
- [ ] 필수 메서드 목록 확인

---

#### 6. 실행 환경 정보
```bash
# 확인 파일
test/environ.json
requirements.txt
setup.py

# 목적
- Python 버전
- 의존성 라이브러리
- 지원 장치 목록
```

**체크 항목:**
- [ ] Python 버전 확인
- [ ] gSDK 버전 확인
- [ ] environ.json에서 테스트 장치 확인
- [ ] 필요한 라이브러리 확인

---

#### 7. 완전 구현 예시 찾기
```bash
# 목적
- 구현 패턴 학습
- 코드 템플릿 확인
- Best practice 이해
```

**체크 항목:**
- [ ] 완전히 구현된 다른 TC 파일 찾기
- [ ] 해당 파일의 패턴 분석:
  - [ ] Capability 체크 방식
  - [ ] EventMonitor 사용법
  - [ ] 백업/복원 패턴
  - [ ] Master/Slave 테스트 방식

---

### 🔹 부가 정보 (Priority 3)

#### 8. 테스트 실행 이력
```bash
# 확인 방법
pytest test/testCOMMONR_XX_*.py -v
python -m unittest test.testCOMMONR_XX_1

# 목적
- 실제 동작 여부
- 에러 발생 위치
- Skip된 테스트 이유
```

**체크 항목:**
- [ ] 테스트 실행 시도
- [ ] 성공/실패/Skip 개수 확인
- [ ] 에러 메시지 분석
- [ ] 실패 원인 파악

---

#### 9. 장치별 Capability 정보
```bash
# 확인 위치
biostar/service/device_pb2.py

# 목적
- 장치 타입별 지원 기능
- Capability 차이 이해
```

**체크 항목:**
- [ ] 장치 타입 목록 확인
- [ ] 각 장치의 지원 기능 파악
- [ ] 버전별 차이 확인

---

## 🔄 정확한 분석 프로세스

### Step 1: 사전 조사 (30분)
1. ✅ 프로젝트 구조 파악
2. ✅ TC 문서 읽기
3. ✅ 기존 코드 검색
4. ✅ 기반 클래스 확인

### Step 2: 코드 분석 (60분)
1. 📄 대상 코드 읽기
2. 🔍 API 호출 검증
3. 📊 TC 커버리지 계산
4. 🐛 논리적 오류 찾기

### Step 3: 비교 분석 (30분, 기존 코드 있는 경우)
1. 📂 기존 코드 읽기
2. ⚖️ 두 코드 비교
3. 🏆 우열 판단
4. 💡 권장 사항 제시

### Step 4: 보고서 작성 (30분)
1. 📋 체크리스트 기반 정리
2. 📈 점수 산정
3. 📝 구체적 개선 방안
4. 🎯 최종 권장 사항

---

## ⚠️ 주의사항

### 절대 평가 vs 상대 평가

#### 절대 평가 (기준 없을 때)
- ✅ TC 문서 대비 커버리지
- ✅ 구조적 규칙 준수
- ✅ 코드 존재 여부

**위험:** 실행 불가능한 코드도 높게 평가될 수 있음

#### 상대 평가 (기존 코드 있을 때)
- ✅ 기존 코드와 API 정확성 비교
- ✅ 실행 가능성 비교
- ✅ 코드 품질 비교

**장점:** 실제 사용 가능 여부 정확히 판단

### 평가 기준 명시

분석 보고서에 항상 포함:
```markdown
## 평가 기준
- [ ] 절대 평가 (TC 문서만 참조)
- [ ] 상대 평가 (기존 코드와 비교)
- 비교 대상: test/testCOMMONR_XX_Y.py
```

---

## 📊 점수 산정 기준

### 절대 평가 시
| 항목 | 배점 | 평가 기준 |
|------|------|----------|
| TC 커버리지 | 30점 | Step 구현 비율 |
| 구조적 정확성 | 20점 | 규칙 준수 여부 |
| 코드 가독성 | 20점 | 주석, docstring |
| 논리적 정확성 | 30점 | 로직 오류 없음 |

### 상대 평가 시
| 항목 | 배점 | 평가 기준 |
|------|------|----------|
| API 정확성 | 30점 | 실제 메서드 사용 |
| 실행 가능성 | 30점 | 동작 여부 |
| 코드 품질 | 20점 | 패턴, 에러 처리 |
| 완성도 | 20점 | TODO, 하드코딩 없음 |

---

## 🎓 학습 내용

### 이번 분석에서 배운 점

1. **프로젝트 구조 파악의 중요성**
   - test/ 와 test/temp/ 모두 확인 필요
   - 어느 것이 실제 사용 코드인지 먼저 파악

2. **API 실제 존재 여부 확인**
   - 코드가 있다고 동작하는 것 아님
   - 기반 클래스 메서드 목록 필수 확인

3. **비교 기준의 명확화**
   - 절대 평가인지 상대 평가인지 명시
   - 비교 대상이 있다면 반드시 비교

---

## 📝 다음 분석 시 적용 사항

### COMMONR-30 분석 전 체크리스트

#### 사전 조사
- [ ] test/testCOMMONR_30_*.py 파일 존재 확인
- [ ] test/temp/testCOMMONR_30_*.py 파일 존재 확인
- [ ] TC_COMMONR/COMMONR-30.txt 읽기
- [ ] 두 폴더 중 어느 것이 실제 코드인지 확인

#### 분석 기준 결정
- [ ] 절대 평가 vs 상대 평가 결정
- [ ] 비교 대상 명시
- [ ] 평가 기준 명확화

#### 실행 가능성 검증
- [ ] 사용된 API가 실제 존재하는지 확인
- [ ] EventMonitor 사용법이 올바른지 확인
- [ ] util 함수 호출이 정확한지 확인

---

## 🔗 참고 링크

### 내부 문서
- [CLAUDE.md](CLAUDE.md) - 프로젝트 규칙
- [testCOMMONR.py](test/testCOMMONR.py) - 기반 클래스
- [util.py](test/util.py) - 유틸리티 함수

### 완벽한 구현 예시
- [testCOMMONR_21_6.py](test/testCOMMONR_21_6.py) - Blacklist
- [testCOMMONR_21_7.py](test/testCOMMONR_21_7.py) - Unblock
- [testCOMMONR_21_10.py](test/testCOMMONR_21_10.py) - Duress

---

**작성일:** 2025-10-14
**버전:** 1.0
**작성자:** Claude Code 분석 시스템
