import chromadb
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from typing import List, Dict, Any, Optional
import json
import requests
import warnings
import datetime

# FutureWarning ë¬´ì‹œ
warnings.filterwarnings("ignore", category=FutureWarning)


class LMStudioLLM(LLM):
    """LM Studioì™€ ì—°ë™í•˜ëŠ” LangChain í˜¸í™˜ LLM í´ë˜ìŠ¤"""
    
    base_url: str = "http://127.0.0.1:1234/v1"
    model_name: str = "qwen/qwen3-8b"
    temperature: float = 0.1
    max_tokens: int = 2048
    
    def __init__(self, 
                 base_url: str = "http://127.0.0.1:1234/v1",
                 model_name: str = "qwen/qwen3-8b",
                 temperature: float = 0.1,
                 max_tokens: int = 2048,
                 **kwargs):
        super().__init__(**kwargs)
        self.base_url = base_url.rstrip('/')
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self._test_connection()
    
    def _test_connection(self):
        """LM Studio ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.get(f"{self.base_url}/models", timeout=5)
            if response.status_code == 200:
                models = response.json()
                print(f"âœ… LM Studio ì—°ê²° ì„±ê³µ! ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(models.get('data', []))}ê°œ")
            else:
                print(f"âš ï¸ LM Studio ì—°ê²° ìƒíƒœ í™•ì¸ í•„ìš”: {response.status_code}")
        except Exception as e:
            print(f"âŒ LM Studio ì—°ê²° ì‹¤íŒ¨: {e}")
    
    @property
    def _llm_type(self) -> str:
        return "lm_studio"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        try:
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "stream": False
            }
            
            if stop:
                payload["stop"] = stop
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=10000000  # 1000ì´ˆ (16ë¶„ 40ì´ˆ)ë¡œ ì„¤ì •
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"Error: LM Studio ì‘ë‹µ ì˜¤ë¥˜ (status: {response.status_code})"
                
        except Exception as e:
            return f"Error: LM Studio í†µì‹  ì˜¤ë¥˜ - {str(e)}"


class LangChainTestCaseSystem:
    def __init__(self, 
                 persist_directory: str = "./chroma_db",
                 collection_name: str = "jira_test_cases",
                 embedding_model_name: str = "intfloat/multilingual-e5-large",
                 lm_studio_url: str = "http://127.0.0.1:1234/v1",
                 lm_studio_model: str = "qwen/qwen3-8b"):
        
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (CPU ì‚¬ìš©)
        model_kwargs = {'device': 'cpu', 'trust_remote_code': True}
        encode_kwargs = {'normalize_embeddings': True, 'batch_size': 4}
        
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        
        # ChromaDB ì—°ê²°
        self.vectorstore = self._connect_to_chroma()
        
        # LM Studio LLM ì´ˆê¸°í™”
        self.llm = LMStudioLLM(
            base_url=lm_studio_url,
            model_name=lm_studio_model,
            temperature=0.1,
            max_tokens=2048
        )
        
        # í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì°¾ê¸°ìš© ì²´ì¸
        self.search_chain = self._setup_search_chain()
        
        # í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„±ìš© ì²´ì¸
        self.generation_chain = self._setup_generation_chain()
        
        # ê²°ê³¼ ì €ì¥ìš©
        self.test_results = []
    
    def _connect_to_chroma(self) -> Chroma:
        """ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ì— ì—°ê²°"""
        try:
            vectorstore = Chroma(
                collection_name=self.collection_name,
                embedding_function=self.embeddings,
                persist_directory=self.persist_directory
            )
            print(f"âœ… ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ '{self.collection_name}' ì—°ê²° ì™„ë£Œ")
            return vectorstore
        except Exception as e:
            print(f"âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def _setup_search_chain(self) -> RetrievalQA:
        """í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ê²€ìƒ‰ìš© ì²´ì¸ ì„¤ì •"""
        search_prompt_template = PromptTemplate(
            template="""ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ëŠ” í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ì°¾ì•„ì„œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ê²€ìƒ‰ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë“¤:
{context}

ì‚¬ìš©ì ìš”ì²­: {question}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ìš”ì²­ê³¼ ê´€ë ¨ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë“¤ì„ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”
2. ê° í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ì˜ ì´ìŠˆ í‚¤, í…ŒìŠ¤íŠ¸ ë°ì´í„°, í…ŒìŠ¤íŠ¸ ìŠ¤í…, ì˜ˆìƒ ê²°ê³¼ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
3. ì´ìŠˆë³„ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ë³´ì—¬ì£¼ì„¸ìš”
4. ì°¾ì€ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ì¶”ê°€ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”

ë‹µë³€:""",
            input_variables=["context", "question"]
        )
        
        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={"score_threshold": 0.6, "k": 10}
            ),
            chain_type_kwargs={"prompt": search_prompt_template},
            return_source_documents=True
        )
    
    def _setup_generation_chain(self) -> RetrievalQA:
        """í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„±ìš© ì²´ì¸ ì„¤ì •"""
        generation_prompt_template = PromptTemplate(
            template="""ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ì¡´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë“¤ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì°¸ê³ í•  ê¸°ì¡´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë“¤:
{context}

ì‚¬ìš©ì ìš”ì²­: {question}

ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì‘ì„± ê°€ì´ë“œë¼ì¸:
1. ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” êµ¬ì²´ì ì¸ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”
2. í…ŒìŠ¤íŠ¸ ëª©ì , ì „ì œ ì¡°ê±´, í…ŒìŠ¤íŠ¸ ìŠ¤í…, ì˜ˆìƒ ê²°ê³¼ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•´ì£¼ì„¸ìš”
3. ê¸°ì¡´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ì˜ íŒ¨í„´ê³¼ í˜•ì‹ì„ ì°¸ê³ í•˜ë˜, ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”
4. í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” êµ¬ì²´ì ì´ê³  í˜„ì‹¤ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”
5. Edge Caseë‚˜ ì˜ˆì™¸ ìƒí™©ë„ ê³ ë ¤í•´ì£¼ì„¸ìš”
6. ê°€ëŠ¥í•˜ë‹¤ë©´ ì—¬ëŸ¬ ê°œì˜ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”

í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ í˜•ì‹:
**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: [í…ŒìŠ¤íŠ¸ ì œëª©]**
- **í…ŒìŠ¤íŠ¸ ëª©ì **: [ëª©ì  ì„¤ëª…]
- **ì „ì œ ì¡°ê±´**: [ì‚¬ì „ ì¡°ê±´]
- **í…ŒìŠ¤íŠ¸ ìŠ¤í…**:
  1. [ìŠ¤í… 1]
  2. [ìŠ¤í… 2]
  ...
- **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: [í•„ìš”í•œ ë°ì´í„°]
- **ì˜ˆìƒ ê²°ê³¼**: [ê¸°ëŒ€í•˜ëŠ” ê²°ê³¼]

ë‹µë³€:""",
            input_variables=["context", "question"]
        )
        
        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 8}
            ),
            chain_type_kwargs={"prompt": generation_prompt_template},
            return_source_documents=True
        )
    
    def find_test_cases(self, query: str, filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì°¾ê¸° ê¸°ëŠ¥"""
        try:
            print(f"ğŸ” í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ê²€ìƒ‰ ì¤‘: {query}")
            
            formatted_query = f"query: {query}"
            response = self.search_chain({"query": formatted_query})
            
            # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë¦¬
            source_docs = []
            for doc in response.get("source_documents", []):
                source_docs.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "issue_key": doc.metadata.get("issue_key", ""),
                    "step_index": doc.metadata.get("step_index", "")
                })
            
            result = {
                "query": query,
                "answer": response["result"],
                "found_test_cases": source_docs,
                "total_found": len(source_docs)
            }
            
            # ê²°ê³¼ ì €ì¥
            self._save_result("find_test_cases", query, result)
            
            return result
            
        except Exception as e:
            error_msg = f"í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            print(f"âŒ {error_msg}")
            error_result = {"query": query, "error": error_msg}
            self._save_result("find_test_cases", query, error_result)
            return error_result
    
    def generate_test_case(self, requirement: str) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± ê¸°ëŠ¥"""
        try:
            print(f"ğŸš€ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± ì¤‘: {requirement}")
            
            formatted_requirement = f"query: {requirement}"
            response = self.generation_chain({"query": formatted_requirement})
            
            # ì°¸ê³ í•œ ì†ŒìŠ¤ ë¬¸ì„œë“¤
            reference_docs = []
            for doc in response.get("source_documents", []):
                reference_docs.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "issue_key": doc.metadata.get("issue_key", ""),
                    "step_index": doc.metadata.get("step_index", "")
                })
            
            result = {
                "requirement": requirement,
                "generated_test_case": response["result"],
                "reference_test_cases": reference_docs,
                "reference_count": len(reference_docs)
            }
            
            # ê²°ê³¼ ì €ì¥
            self._save_result("generate_test_case", requirement, result)
            
            return result
            
        except Exception as e:
            error_msg = f"í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
            print(f"âŒ {error_msg}")
            error_result = {"requirement": requirement, "error": error_msg}
            self._save_result("generate_test_case", requirement, error_result)
            return error_result
    
    def _save_result(self, test_type: str, query: str, result: Any):
        """ê²°ê³¼ ì €ì¥"""
        self.test_results.append({
            "timestamp": datetime.datetime.now().isoformat(),
            "framework": "langchain",
            "test_type": test_type,
            "query": query,
            "result": result
        })
    
    def save_results_to_file(self, filename: str = None):
        """ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥"""
        if filename is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"langchain_testcase_results_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… LangChain ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def print_find_result(self, result: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì°¾ê¸° ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ“‹ ê²€ìƒ‰ ê²°ê³¼:")
        print("=" * 50)
        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
            return
        
        print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {result['query']}")
        print(f"ğŸ“Š ì°¾ì€ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìˆ˜: {result['total_found']}ê°œ")
        print(f"\nğŸ’¡ ë¶„ì„ ê²°ê³¼:")
        print(result['answer'])
        
        if result['found_test_cases']:
            print(f"\nğŸ“š ì°¾ì€ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë“¤:")
            for i, doc in enumerate(result['found_test_cases'][:5], 1):
                issue_key = doc['issue_key']
                step_idx = doc['step_index']
                print(f"\n{i}. {issue_key}_step_{step_idx}")
                print(f"   {doc['content'][:150]}...")
    
    def print_generation_result(self, result: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸš€ ìƒì„± ê²°ê³¼:")
        print("=" * 50)
        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
            return
        
        print(f"ğŸ“ ìš”êµ¬ì‚¬í•­: {result['requirement']}")
        print(f"ğŸ“Š ì°¸ê³ í•œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìˆ˜: {result['reference_count']}ê°œ")
        print(f"\nğŸ¯ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤:")
        print(result['generated_test_case'])
    
    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ LangChain í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # 1. í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì°¾ê¸° í…ŒìŠ¤íŠ¸ë“¤
        find_queries = [
            "Master Adminê³¼ ê´€ë ¨ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¤‘ì—ì„œ master admin ì„¤ì • ê°¯ìˆ˜ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ê°€ì ¸ì™€ì¤˜"
        ]
        
        print("\nğŸ” í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì°¾ê¸° í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        for query in find_queries:
            result = self.find_test_cases(query)
            self.print_find_result(result)
            print("\n" + "="*30 + "\n")
        
        # 2. í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸ë“¤
        generation_requirements = [
            "ì¥ì¹˜ì— ì „ì²´ ê´€ë¦¬ì ì„¤ì •ì„ ê°•ì œí•  ìˆ˜ ìˆëŠ” Master Admin ê¸°ëŠ¥ì´ ìˆëŠ”ë° ì´ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘ì„ í•´. ë‹¤ë§Œ ì´ ê¸°ëŠ¥ì´ ë™ì‘ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì¡°ê±´ì´ ìˆì–´. ë²„ì „ì´ V1.4.0 ì´ìƒìœ¼ë¡œ ìƒì‚°ëœ ì œí’ˆì´ì–´ì•¼í•´. ì¡°ê±´ì— ë¶€í•©ë˜ëŠ” ì¥ì¹˜ì˜ ì „ì›ì´ ì¸ê°€ë˜ë©´ í™”ë©´ì— Master Admin ì„¤ì •í™”ë©´ì´ í‘œì‹œê°€ ë¼. í•˜ì§€ë§Œ ë²„ì „ì´ V1.4.0 ì´í•˜ë¡œ ìƒì‚°ëœ ì œí’ˆì˜ ê²½ìš°ì—ëŠ” ì¥ì¹˜ ì „ì›ì´ ì¸ê°€ë˜ë©´ ë©”ì¸í™”ë©´ì´ í‘œì‹œê°€ ë¼. ë²„ì „ì€ BS3ì˜ ì´ì „ ë²„ì „ë“¤ì„ ì°¸ê³ í•´ì„œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ì‘ì„±í•´ì¤˜."
        ]
        
        print("\nğŸš€ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        for requirement in generation_requirements:
            result = self.generate_test_case(requirement)
            self.print_generation_result(result)
            print("\n" + "="*30 + "\n")
        
        # ê²°ê³¼ ì €ì¥
        self.save_results_to_file()
        
        print(f"\nğŸ‰ LangChain ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ì´ {len(self.test_results)}ê°œì˜ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # LangChain í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    langchain_system = LangChainTestCaseSystem(
        lm_studio_url="http://127.0.0.1:1234/v1",
        lm_studio_model="qwen/qwen3-8b"
    )
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    langchain_system.run_comprehensive_test()